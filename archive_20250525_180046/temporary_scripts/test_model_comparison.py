"""
Comparison script for all trained models.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.absolute()
sys.path.append(str(project_root))

from config import settings
from utils import io
from evaluation.metrics import load_all_models

def compare_all_models():
    """Compare all trained models and visualize the results."""
    # Load all models
    print("Loading all trained models...")
    all_models = load_all_models()
    
    if not all_models:
        print("No models found. Please train models first.")
        return
    
    print(f"Found {len(all_models)} trained models.")
    
    # Create a comparison DataFrame
    model_metrics = []
    for model_name, model_data in all_models.items():
        model_type = model_data.get('model_type', 'Unknown')
        if 'XGB_' in model_name:
            model_family = 'XGBoost'
            optuna_optimized = 'optuna' in model_name
        elif 'LightGBM_' in model_name:
            model_family = 'LightGBM'
            optuna_optimized = 'optuna' in model_name
        elif 'ElasticNet' in model_name:
            model_family = 'ElasticNet'
            optuna_optimized = True  # ElasticNet is always optimized
        else:
            model_family = 'Linear Regression'
            optuna_optimized = False
        
        # Get dataset name
        if 'LR_' in model_name:
            dataset = model_name.replace('LR_', '')
        elif 'ElasticNet_LR_' in model_name:
            dataset = model_name.replace('ElasticNet_LR_', '')
        elif 'XGB_' in model_name:
            dataset = model_name.replace('XGB_', '').split('_')[0]
        elif 'LightGBM_' in model_name:
            dataset = model_name.replace('LightGBM_', '').split('_')[0]
        else:
            dataset = 'Unknown'
        
        metrics = {
            'model_name': model_name,
            'model_family': model_family,
            'model_type': model_type,
            'dataset': dataset,
            'optuna_optimized': optuna_optimized,
            'RMSE': model_data.get('RMSE', np.sqrt(model_data.get('MSE', 0))),
            'MAE': model_data.get('MAE', 0),
            'MSE': model_data.get('MSE', 0),
            'R2': model_data.get('R2', 0),
            'n_companies': model_data.get('n_companies', 0),
            'n_features': model_data.get('n_features', 0)
        }
        model_metrics.append(metrics)
    
    # Convert to DataFrame
    metrics_df = pd.DataFrame(model_metrics)
    
    # Save to CSV
    io.ensure_dir(settings.METRICS_DIR)
    metrics_df.to_csv(f"{settings.METRICS_DIR}/all_models_comparison.csv", index=False)
    
    # Create output directory for plots - place directly in performance directory
    output_dir = settings.VISUALIZATION_DIR / "performance"
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # This function has been simplified to not create model comparison visualizations
    # because these visualizations are created in the new visualization architecture.
    # The comparison metrics data has been saved to CSV for reference.
    
    print("\nStoring model comparison data in CSV format only. Visualizations will be generated by the new architecture.")
    
    print("\nModel comparison completed. Results saved to metrics directory and visualization plots saved.")
    return metrics_df

if __name__ == "__main__":
    # Run the comparison
    compare_all_models()